{
  "metadata": {
    "name": "tlc_profiling",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, LongType}\nimport spark.implicits._"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Define Spark Session\nval spark \u003d SparkSession.builder().appName(\"tlcTripRecordETL\").getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def loadRawTLCDataParquet(spark: SparkSession, path: String): DataFrame\u003d {\n    spark.read.parquet(s\"$path.parquet\")\n  }\n \n  \ndef saveCleanData(resultDF: DataFrame, cleanOutputPath: String): Unit \u003d {\n    resultDF.write.mode(SaveMode.Overwrite).parquet(s\"$cleanOutputPath\")\n  }\n  \n  \ndef loadCleanData(path: String): DataFrame \u003d {\n    spark.read.parquet(f\"$path\")\n  }\n  \n\ndef getNumericColumnLowerAndUpperBound(df: DataFrame, columnName: String) \u003d {\n    val meanValue \u003d df.agg(stddev(columnName).alias(\"stddev\"), mean(columnName).alias(\"mean\")).head()\n    val stddev_ \u003d meanValue.getDouble(0)\n    val mean_ \u003d meanValue.getDouble(1)\n    // Define the upper and lower bounds\n    val lowerBound \u003d mean_ - 3 * stddev_\n    val upperBound \u003d mean_ + 3 * stddev_\n    (lowerBound, upperBound)\n}\n\n\ndef cleanRawDataYellow(rawDF: DataFrame): DataFrame \u003d {\n\n    // Keep non-null entries in these columns\n    val nonNullDF \u003d rawDF.filter(\n        col(\"PUlocationID\").isNotNull \u0026\u0026\n        col(\"DOlocationID\").isNotNull \u0026\u0026\n        col(\"total_amount\").isNotNull \u0026\u0026\n        col(\"fare_amount\").isNotNull \u0026\u0026\n        col(\"payment_type\").isNotNull \u0026\u0026\n        col(\"RatecodeID\").isNotNull)\n    \n    // Rename columns\n    val newColumnNames \u003d Map(\n      \"tpep_dropoff_datetime\" -\u003e \"dropoff_datetime\",\n      \"tpep_pickup_datetime\" -\u003e \"pickup_datetime\",\n      \"PUlocationID\" -\u003e \"pulocationID\",\n      \"DOlocationID\" -\u003e \"dolocationID\"\n    )\n        \n    val renamedDF \u003d newColumnNames.foldLeft(nonNullDF) {\n        case (accDF, (oldName, newName)) \u003d\u003e accDF.withColumnRenamed(oldName, newName)\n    }\n    \n    // Compute valid ranges of values for columns\n    val validPaymentTypes \u003d Seq(1, 2, 3)\n    val validRateCodeIds \u003d Seq(1, 5, 6)\n    val validTotalAmount \u003d getNumericColumnLowerAndUpperBound(rawDF, \"total_amount\")\n    val totalAmountLower \u003d validTotalAmount._1\n    val totalAmountUpper \u003d validTotalAmount._2\n    val pattern \u003d \"\"\"(\\d{4}-\\d{2})\"\"\".r\n    \n    // Add month-date from data and compare it with month-date extracted from data file name\n    val transformedDF \u003d renamedDF.withColumn(\"year_month\", regexp_extract(col(\"file_path\"), pattern.toString, 1)).withColumn(\"data_year_month\", concat_ws(\"-\", year(col(\"pickup_datetime\")), format_string(\"%02d\", month(col(\"pickup_datetime\")))))\n    \n    // Remove rows which follow rules\n    val filteredDF \u003d transformedDF.filter(col(\"passenger_count\") \u003d!\u003d 0).filter(col(\"pulocationID\") \u003d!\u003d col(\"dolocationID\")).filter(col(\"data_year_month\") \u003d\u003d\u003d col(\"year_month\")).filter(col(\"payment_type\").isin(validPaymentTypes: _*)).filter(col(\"RatecodeID\").isin(validRateCodeIds: _*)).filter(col(\"fare_amount\") \u003d!\u003d 0).filter(col(\"total_amount\").between(totalAmountLower, totalAmountUpper))\n    \n    // Impute: cast to Long\n    val castDf \u003d filteredDF.withColumn(\"pulocationID\", col(\"pulocationID\").cast(LongType)).withColumn(\"dolocationID\", col(\"dolocationID\").cast(LongType))\n    castDf.select(\"pulocationID\", \"dolocationID\").coalesce(1)\n}\n  \n \ndef ETLYellowTaxiTripDataset(spark: SparkSession, path: String, cleanOutputPath:String): Unit \u003d {\n    val years \u003d Array(\"2020\", \"2021\", \"2022\", \"2023\")\n    val months \u003d Array(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\")\n    \n    val filePaths2020 \u003d for (m \u003c- months.slice(9, 12)) yield s\"$path/2020/$m/yellow_tripdata_2020-${m}\"\n    val filePaths2021 \u003d for (m \u003c- months) yield s\"$path/2021/${m}/yellow_tripdata_2021-${m}\"\n    val filePaths2022 \u003d for (m \u003c- months) yield s\"$path/2022/${m}/yellow_tripdata_2022-${m}\"\n    val filePaths2023 \u003d for (m \u003c- months.slice(0, 9)) yield s\"$path/2023/${m}/yellow_tripdata_2023-${m}\"\n    \n    val filePaths \u003d filePaths2020 ++ filePaths2021 ++ filePaths2022 ++ filePaths2023\n    \n    val schema \u003d StructType(Array(StructField(\"pulocationID\", LongType, true), StructField(\"dolocationID\", LongType, true)))\n    var resultDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\n\n    for (filePath \u003c- filePaths) {\n        val rawDF \u003d loadRawTLCDataParquet(spark, filePath)\n        val transformedDF \u003d rawDF.withColumn(\"file_path\", lit(filePath))\n        val cleanDF \u003d cleanRawDataYellow(transformedDF)\n        resultDF \u003d resultDF.union(cleanDF)\n    }\n    \n    saveCleanData(resultDF, cleanOutputPath)\n    loadCleanData(cleanOutputPath).show()\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val sourcePath \u003d \"/user/cg4177_nyu_edu/project/data/source/tlc/yellow\"\nval cleanOutputPath \u003d \"/user/cg4177_nyu_edu/project/data/clean/tlc/yellow/merged_yellow_cleaned_data.parquet\"\nETLYellowTaxiTripDataset(spark, sourcePath, cleanOutputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def cleanRawDataGreen(rawDF: DataFrame): DataFrame \u003d {\n\n    // Keep non-null entries in these columns\n    val nonNullDF \u003d rawDF.filter(\n        col(\"PUlocationID\").isNotNull \u0026\u0026\n        col(\"DOlocationID\").isNotNull \u0026\u0026\n        col(\"total_amount\").isNotNull \u0026\u0026\n        col(\"fare_amount\").isNotNull \u0026\u0026\n        col(\"payment_type\").isNotNull \u0026\u0026\n        col(\"RatecodeID\").isNotNull)\n    \n    // Rename columns\n    val newColumnNames \u003d Map(\n      \"lpep_pickup_datetime\" -\u003e \"dropoff_datetime\",\n      \"lpep_dropoff_datetime\" -\u003e \"pickup_datetime\",\n      \"PUlocationID\" -\u003e \"pulocationID\",\n      \"DOlocationID\" -\u003e \"dolocationID\"\n    )\n        \n    val renamedDF \u003d newColumnNames.foldLeft(nonNullDF) {\n        case (accDF, (oldName, newName)) \u003d\u003e accDF.withColumnRenamed(oldName, newName)\n    }\n    \n    // Compute valid ranges of values for columns\n    val validPaymentTypes \u003d Seq(1, 2, 3)\n    val validRateCodeIds \u003d Seq(1, 5, 6)\n    val validTotalAmount \u003d getNumericColumnLowerAndUpperBound(rawDF, \"total_amount\")\n    val totalAmountLower \u003d validTotalAmount._1\n    val totalAmountUpper \u003d validTotalAmount._2\n    val pattern \u003d \"\"\"(\\d{4}-\\d{2})\"\"\".r\n\n    \n    // Add month-date from data and compare it with month-date extracted from data file name\n    val transformedDF \u003d renamedDF.withColumn(\"year_month\", regexp_extract(col(\"file_path\"), pattern.toString, 1)).withColumn(\"data_year_month\", concat_ws(\"-\", year(col(\"pickup_datetime\")), format_string(\"%02d\", month(col(\"pickup_datetime\")))))\n    \n    // Remove rows which follow rules\n    val filteredDF \u003d transformedDF.filter(col(\"passenger_count\") \u003d!\u003d 0).filter(col(\"pulocationID\") \u003d!\u003d col(\"dolocationID\")).filter(col(\"data_year_month\") \u003d\u003d\u003d col(\"year_month\")).filter(col(\"payment_type\").isin(validPaymentTypes: _*)).filter(col(\"RatecodeID\").isin(validRateCodeIds: _*)).filter(col(\"fare_amount\") \u003d!\u003d 0).filter(col(\"total_amount\").between(totalAmountLower, totalAmountUpper))\n    \n    // Impute: cast to Long\n    val castDf \u003d filteredDF.withColumn(\"pulocationID\", col(\"pulocationID\").cast(LongType)).withColumn(\"dolocationID\", col(\"dolocationID\").cast(LongType))\n    castDf.select(\"pulocationID\", \"dolocationID\").coalesce(1)\n}\n  \n \ndef ETLGreenTaxiTripDataset(spark: SparkSession, path: String, cleanOutputPath:String): Unit \u003d {\n    val years \u003d Array(\"2020\", \"2021\", \"2022\", \"2023\")\n    val months \u003d Array(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\")\n    \n    val filePaths2020 \u003d for (m \u003c- months.slice(9, 12)) yield s\"$path/2020/$m/green_tripdata_2020-${m}\"\n    val filePaths2021 \u003d for (m \u003c- months) yield s\"$path/2021/${m}/green_tripdata_2021-${m}\"\n    val filePaths2022 \u003d for (m \u003c- months) yield s\"$path/2022/${m}/green_tripdata_2022-${m}\"\n    val filePaths2023 \u003d for (m \u003c- months.slice(0, 9)) yield s\"$path/2023/${m}/green_tripdata_2023-${m}\"\n    \n    val filePaths \u003d filePaths2020 ++ filePaths2021 ++ filePaths2022 ++ filePaths2023\n\n    val schema \u003d StructType(Array(StructField(\"pulocationID\", LongType, true), StructField(\"dolocationID\", LongType, true)))\n    var resultDF \u003d spark.createDataFrame(spark.sparkContext.emptyRDD[Row], schema)\n\n    for (filePath \u003c- filePaths) {\n        val rawDF \u003d loadRawTLCDataParquet(spark, filePath)\n        val transformedDF \u003d rawDF.withColumn(\"file_path\", lit(filePath))\n        val cleanDF \u003d cleanRawDataGreen(transformedDF)\n        resultDF \u003d resultDF.union(cleanDF)\n    }\n    \n    saveCleanData(resultDF, cleanOutputPath)\n    loadCleanData(cleanOutputPath).show()\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val sourcePath \u003d \"/user/cg4177_nyu_edu/project/data/source/tlc/green\"\nval cleanOutputPath \u003d \"/user/cg4177_nyu_edu/project/data/clean/tlc/green/merged_green_cleaned_data.parquet\"\nETLGreenTaxiTripDataset(spark, sourcePath, cleanOutputPath)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Data Profiling for TLC Yellow and Green Taxi Trip Record Data\n\nThree options:\n* Summary of cleaned data\n* Distinct Values\n* Frequency Distribution and Value Counts\n\nObservations - \n* Do Green Taxis pick up from Lower Manhattan and Midtown?\n* What are the top-5 pick-up locations for Yellow and Green Taxis?\n* What\u0027s the most frequent route for Yellow and Green Taxis?"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val tlcYellowCleanDF \u003d loadCleanData(\"/user/cg4177_nyu_edu/project/data/clean/tlc/yellow/merged_yellow_cleaned_data.parquet\")\nval tlcGreenCleanDF \u003d loadCleanData(\"/user/cg4177_nyu_edu/project/data/clean/tlc/green/merged_green_cleaned_data.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(tlcYellowCleanDF.summary())"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(tlcGreenCleanDF.summary())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Distinct Counts"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Distinct Pick-Up and Drop-Off Location IDs in the Dataset\n\nval tlcYellowpulocationIDs: Array[Any] \u003d tlcYellowCleanDF.select(\"pulocationID\").distinct().orderBy(\"pulocationID\").collect().map(row \u003d\u003e row(0))\nval tlcYellowdolocationIDs: Array[Any] \u003d tlcYellowCleanDF.select(\"dolocationID\").distinct().orderBy(\"dolocationID\").collect().map(row \u003d\u003e row(0))\n\nval tlcGreenpulocationIDs: Array[Any] \u003d tlcGreenCleanDF.select(\"pulocationID\").distinct().orderBy(\"pulocationID\").collect().map(row \u003d\u003e row(0))\nval tlcGreendolocationIDs: Array[Any] \u003d tlcGreenCleanDF.select(\"dolocationID\").distinct().orderBy(\"dolocationID\").collect().map(row \u003d\u003e row(0))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Green taxis are allowed to operate in above W 110 St/E 96th St in Manhattan\nval manhattanAreas \u003d Array(4,24,12,13,41,45,42,43,48,50,68,79,74,75,87,88,90,125,100,103,103,103,107,113,114,116,120,127,128,151,140,137,141,142,152,143,144,148,153,158,161,162,163,164,170,166,186,194,202,209,211,224,229,230,231,239,232,233,234,236,237,238,263,243,244,246,249,261,262)\nval greenTaxiManhattanValidAreas \u003d Array(166, 41, 74, 75, 152, 42, 116, 244, 120, 243, 128, 127, 153)\nval greenTaxiExcludedAreas \u003d manhattanAreas.filterNot(greenTaxiManhattanValidAreas.contains)\n\n// Lets check if the pick-up and drop-off areas intersect for Green Taxis\nval greenTaxiViolatedAreas \u003d (tlcGreenpulocationIDs.intersect(greenTaxiExcludedAreas) ++ tlcGreendolocationIDs.intersect(greenTaxiExcludedAreas)).toSet\n\nif (greenTaxiViolatedAreas.nonEmpty) {\n  println(\"Green taxis violate pick-up and drop-off in these Manhattan areas: \" + greenTaxiViolatedAreas.mkString(\", \"))\n} else {\n  println(\"Green Taxis operate in designated areas\")\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We observe that Green Taxis violate the rules of operation below W 110 St/E 96th St in Manhattan"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The dataset contains pick-up and dropoff from all locations within all the 5 boroughs. These numbers are representative of the areas within the boroughs. For detailed info refer - \n* https://www.nyc.gov/assets/tlc/images/content/pages/about/taxi_zone_map_manhattan.jpg\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Frequency Distribution and Value Counts"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Top-5 pick-up locations for Yellow Taxis and Green Taxis\n\nval columnName \u003d \"pulocationID\"\nval top5pulocationIDYellow \u003d tlcYellowCleanDF.groupBy(columnName)\n  .agg(\n      count(columnName).alias(\"trips\"),\n      round((count(columnName) / tlcYellowCleanDF.count())*100.0, 2).alias(\"percentage\")\n      )\n  .orderBy(col(\"trips\").desc)\n  .limit(5)\n  \nval top5pulocationIDGreen \u003d tlcGreenCleanDF.groupBy(columnName)\n  .agg(\n      count(columnName).alias(\"trips\"),\n      round((count(columnName) / tlcGreenCleanDF.count())*100.0, 2).alias(\"percentage\")\n      )\n  .orderBy(col(\"trips\").desc)\n  .limit(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "println(\"Top-5 Pick-Up locations for Yellow Taxis: \")\ntop5pulocationIDYellow.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "println(\"Top-5 Pick-Up locations for Green Taxis: \")\ntop5pulocationIDGreen.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "* Roughly 20% of Yellow Taxis pickup from Midtown and Upper East Side next to Central Park in Manhattan\n* About 50% of Green Taxis pickup from Upper East Side above Central Park and from above 110th St W in Manhattan. The next popular pickup stop is in Queens"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Most popular Yellow and Green Taxi Trip\nval column1 \u003d \"pulocationID\"\nval column2 \u003d \"dolocationID\"\n\nval topRouteYellowTaxi \u003d tlcYellowCleanDF.withColumn(\"trip_route\", when(col(column1) \u003c col(column2), array(column1, column2)).otherwise(array(column2, column1)))\n    .groupBy(\"trip_route\")\n    .agg(\n        count(\"*\").alias(\"counts\"),\n        (count(\"*\") / tlcYellowCleanDF.count() * 100).alias(\"percentage\")\n        )\n    .orderBy(col(\"counts\").desc)\n    .limit(1)\n\nval topRouteGreenTaxi \u003d tlcGreenCleanDF.withColumn(\"trip_route\", when(col(column1) \u003c col(column2), array(column1, column2)).otherwise(array(column2, column1)))\n    .groupBy(\"trip_route\")\n    .agg(\n        count(\"*\").alias(\"counts\"),\n        (count(\"*\") / tlcGreenCleanDF.count() * 100).alias(\"percentage\")\n        )\n    .orderBy(col(\"counts\").desc)\n    .limit(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "topRouteYellowTaxi.show();\ntopRouteGreenTaxi.show();"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Most popular trip route for both taxis is in Upper East Side Manhattan"
    }
  ]
}