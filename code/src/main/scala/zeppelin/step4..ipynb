{
  "metadata": {
    "name": "step4",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, LongType, StringType, IntegerType, ArrayType}\nimport spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.sql.functions.{count, desc}\nimport org.apache.spark.sql.{Dataset, SaveMode, SparkSession}"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Define Spark Session\nval spark \u003d SparkSession.builder().appName(\"tlcTripRecordETL\").getOrCreate()\nval sc \u003d spark.sparkContext"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// val trip_path \u003d Array(\"72,45\", \"65,78\", \"72,45,15,65,78\", \"24,70,49\", \"12,24,70,49\", \"45,72\")\n// val frequency \u003d Array(2, 3, 4, 5, 1, 3)\n\n// val rows \u003d frequency.zip(trip_path)\n\n// val inputRDD \u003d sc.parallelize(rows)\n// inputRDD.collect().foreach(println)\n\n// // Convert RDD to TSV format\n// val tsvRDD \u003d inputRDD.map { case (frequency, trip_path) \u003d\u003e s\"$frequency\\t$trip_path\" }\n// tsvRDD.collect().foreach(println)\n\n// // Save RDD as TSV file\n// tsvRDD.saveAsTextFile(\"/user/cg4177_nyu_edu/step4.tsv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def loadRawDataCSV(spark: SparkSession, path: String, headers: Boolean \u003d true, inferSchema: Boolean \u003d true,\n                 delimiter: String \u003d \",\"): DataFrame \u003d {\nspark.read\n  .option(\"header\", headers)\n  .option(\"inferSchema\", inferSchema)\n  .option(\"delimiter\", delimiter)\n  .csv(path)\n}\n\ncase class TripPathFreq(frequency: BigInt, trip_path: String)\n\nval path \u003d \"/user/wl2484_nyu_edu/project/data/intermediate/taxi_trip_path_frequency/part-00000-f0abcd69-ecbe-4170-bd8c-69830eb9ba73-c000.csv\"\n\nval df \u003d loadRawDataCSV(spark, path, delimiter \u003d \"\\t\")\ndf.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0)))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// // // Read the .tsv file into an RDD of Strings\n// val linesRDD \u003d sc.textFile(\"/user/cg4177_nyu_edu/step4.tsv\")\n// val tripFreqRDD \u003d linesRDD.map { line \u003d\u003e\n//   // Assuming the key is the first column and the value is the second column\n//   val Array(value, key) \u003d line.split(\"\\t\")\n//   (key, value.toInt)\n// }"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// val pathFreqRDD \u003d sc.parallelize( df.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0))).take(100))\nval pathFreqRDD \u003ddf.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0)))\n\n// val pathFreqRDD \u003d tripFreqRDD\nval trips \u003d pathFreqRDD.keys.collect()\nprintln(trips.length)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathFreqRDD.map(_._2).sum()\npathFreqRDD.take(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.mutable.Map\nimport scala.collection.mutable.HashMap\n\n/*\n *  Implements a standard Union-Find (a.k.a Disjoint Set) data\n *  structure with permissive behavior with respect to\n *  non-existing elements in the structure (Unknown elements are\n *  added as new elements when queried for).\n *\n * See Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald\n * L.; Stein, Clifford (2001), \"Chapter 21: Data structures for\n * Disjoint Sets\", Introduction to Algorithms (Second ed.), MIT\n * Press, pp. 498–524, ISBN 0-262-03293-7\n *\n * Amortized time for a sequence of m {union, find} operations\n * is O(m * InvAckermann(n)) where n is the number of elements\n * and InvAckermann is the inverse of the Ackermann function.\n *\n * Not thread-safe.\n */\n\nclass UnionFind[T]() {\n\n  var parent: Map[T, T] \u003d new HashMap[T,T] {\n    override def default(s: T) \u003d {\n        get(s) match {\n          case Some(v) \u003d\u003e v\n          case None    \u003d\u003e put(s, s); s\n        }\n    }\n  }\n\n  var rank: Map[T, Int] \u003d new HashMap[T,Int] {\n    override def default(s: T) \u003d {\n        get(s) match {\n          case Some(v) \u003d\u003e v\n          case None    \u003d\u003e put(s, 1); 1\n        }\n    }\n  }\n\n  /**\n   * Return the parent (representant) of the equivalence class.\n   * Uses path compression.\n   */\n  def find(s: T): T \u003d {\n    val ps \u003d parent(s)\n    if (ps \u003d\u003d s) s else {\n      val cs \u003d find(ps)\n      parent(s) \u003d cs\n      cs\n    }\n  }\n\n  /**\n   *  Unify equivalence classes of elements.\n   *  Uses union by rank.\n   */\n  def union(x: T, y: T): Unit \u003d {\n    val cx \u003d find(x)\n    val cy \u003d find(y)\n    if (cx !\u003d cy) {\n      val rx \u003d rank(x)\n      val ry \u003d rank(y)\n      if (rx \u003e ry) parent(cy) \u003d cx\n      else if (rx \u003c ry) parent(cx) \u003d cy\n      else {\n        rank(cx) +\u003d 1\n        parent(cy) \u003d cx\n      }\n    }\n  }\n\n  /**\n   * Enumerates the equivalence class of element x\n   */\n  def equivalenceClass(x: T): List[T] \u003d {\n    val px \u003d parent(x)\n    parent.keys filter (parent(_:T) \u003d\u003d px) toList\n  }\n\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val n \u003d trips.length\n\nval ufString \u003d new UnionFind[String]\n\ndef isSubsequence(s1:String, s2: String): Boolean \u003d {\n    val arr1 \u003d s1.split(\",\").map(_.toInt)\n    val arr2 \u003d s2.split(\",\").map(_.toInt)\n    var i \u003d 0  // Index for arr1\n    var j \u003d 0  // Index for arr2\n\n     while (i \u003c arr1.length \u0026\u0026 j \u003c arr2.length) {\n        if (arr1(i) \u003d\u003d arr2(j)) {\n            i +\u003d 1\n        }\n        j +\u003d 1\n    }\n\n    // If all elements of arr1 are found in arr2 in the same order\n    i \u003d\u003d arr1.length\n}\n\nfor (i \u003c- 0 until n) {\n    for (j \u003c- i+1 until n) {\n        val trip1 \u003d trips(i)\n        val trip2 \u003d trips(j)\n        if (isSubsequence(trip1, trip2)) {\n            // println(s\"Trip1: $trip1 is a subsequence of Trip2: $trip2\")\n            ufString.union(trip2, trip1)\n        }\n    }\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "ufString.parent"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var rowsOutput \u003d Seq.empty[(String, String)]\nfor (trip \u003c- trips){\n    val parentTrip \u003d ufString.find(trip)\n    rowsOutput \u003d rowsOutput :+ (trip -\u003e parentTrip)\n}\n\nval pathParentRDD \u003d spark.sparkContext.parallelize(rowsOutput)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// pathParentRDD.collect().foreach(println)\n// println(\"--------------------------------------------------------------------\")\n// pathFreqRDD.collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "assert (pathParentRDD.count() \u003d\u003d pathFreqRDD.count())"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// // Your existing RDDs (replace these with your RDD creation logic)\n// val rdd2 \u003d sc.parallelize(Seq((\"key1\", \"key1\"), (\"key2\", \"key1\"), (\"key3\", \"key3\"), (\"key4\", \"key1\")))\n// val rdd1 \u003d sc.parallelize(Seq((\"key1\", 10), (\"key2\", 20), (\"key3\", 30), (\"key4\", 40)))\n\n// // Perform left join\n// val joinedRDD \u003d rdd1.leftOuterJoin(rdd2)\n\n// // Display the content of the joinedRDD\n// joinedRDD.collect().foreach(println)\n\n\n// // Perform left outer join and sum up the values\n// val resultRDD \u003d joinedRDD\n//   .map { case (key, (value, opt)) \u003d\u003e (opt.getOrElse(\"byeeee\"), value) }  // Extract key and value\n//   .reduceByKey(_ + _)  // Sum up values for each key\n\n// // Display the content of the resultRDD\n// resultRDD.collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// make trip coverage count rdd\n\n\n// Left join RDD1 and RDD2\nval joinedRDD \u003d pathFreqRDD.leftOuterJoin(pathParentRDD)\n\n// Perform left outer join and sum up the values\nval pathCoverageCountRDD \u003d joinedRDD\n  .map { case (key, (value, opt)) \u003d\u003e (opt.getOrElse(\"byeeee\"), value) }  // Extract key and value\n  .reduceByKey(_ + _)  // Sum up values for each key\n\n// Display the content of the pathCoverageCountRDD\npathCoverageCountRDD"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "assert(pathCoverageCountRDD.map(_._2).sum() \u003d\u003d pathFreqRDD.map(_._2).sum())"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathCoverageCountRDD.collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "case class TripPathCoverageCount(trip_path: String, coverage_count: BigInt)\nval pathCoverageCountDS: Dataset[TripPathCoverageCount] \u003d pathCoverageCountRDD.map({\n      case (trip_path, coverage_count) \u003d\u003e TripPathCoverageCount(trip_path, coverage_count)}).toDS()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathCoverageCountDS.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def savePathCoverageOutput(pathCoverageCountDS: Dataset[TripPathCoverageCount], path: String): Unit \u003d {\n    val toSaveDS \u003d pathCoverageCountDS.select(\"coverage_count\", \"trip_path\").as[TripPathCoverageCount]\n    toSaveDS.coalesce(1)\n      .orderBy(desc(\"coverage_count\"))\n      .write\n      .mode(SaveMode.Overwrite) // workaround for abnormal path-already-exists error\n      .option(\"header\", true)\n      .option(\"delimiter\", \"\\t\")\n      .csv(f\"$path\")\n  }"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pathCoverageOutputPath \u003d \"/user/wl2484_nyu_edu/project/data/intermediate/trip_paths_coverage_count\"\nsavePathCoverageOutput(pathCoverageCountDS, pathCoverageOutputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// verify - \n\nval resultDF \u003d loadRawDataCSV(spark, \"/user/wl2484_nyu_edu/project/data/intermediate/trip_paths_coverage_count\", delimiter \u003d \"\\t\")"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "resultDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lookupPath \u003d \"/user/wl2484_nyu_edu/project/data/source/tlc/zone_lookup\"\n\nval lookupDF \u003d loadRawDataCSV(spark, lookupPath, delimiter\u003d\",\")\nlookupDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.Map\nval lookupMap: Map[Int, String] \u003d  lookupDF.withColumn(\"Zone1\", split(col(\"Zone\"), \"/\")(0)).select(\"LocationID\", \"Zone1\").as[(Int, String)].rdd.collectAsMap()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\"230,100,164\".split(\",\").map(num \u003d\u003e lookupMap.getOrElse(num.toInt, \"NA\")).mkString(\",\")"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def mapLocationIds(input: String, locationIdZoneMap: Map[Int, String]): String \u003d {\n    input.split(\",\").map(num \u003d\u003e locationIdZoneMap.getOrElse(num.toInt, \"NA\")).mkString(\",\")\n}\n\nval mapLocationIdsUDF \u003d udf((input: String) \u003d\u003e mapLocationIds(input, lookupMap))\n\n      \nval resultDF1 \u003d resultDF.withColumn(\"route\", mapLocationIdsUDF(col(\"trip_path\")))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "resultDF1.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def topKTripsWithAtleastMStops(k: Int, m: Int, ds: Dataset[TripPathCoverageCount]): Dataset[TripPathCoverageCount] \u003d {\n    val filteredDS: Dataset[TripPathCoverageCount] \u003d ds.filter({ data \u003d\u003e data.trip_path.split(\",\").length \u003e\u003d m })\n    val sortedDS: Dataset[TripPathCoverageCount] \u003d filteredDS.orderBy($\"coverage_count\".desc)\n    sortedDS.limit(k).as[TripPathCoverageCount]\n  }"
    }
  ]
}