{
  "metadata": {
    "name": "step4-normal",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructType, StructField, LongType, StringType, IntegerType, ArrayType}\nimport spark.implicits._\nimport org.apache.spark.rdd.RDD\nimport scala.collection.Map\n\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.sql.functions.{count, desc}\nimport org.apache.spark.sql.{Dataset, SaveMode, SparkSession}"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Define Spark Session\nval spark \u003d SparkSession.builder().appName(\"tlcTripRecordETL\").getOrCreate()\nval sc \u003d spark.sparkContext"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// val trip_path \u003d Array(\"72,45\", \"65,78\", \"72,45,15,65,78\", \"24,70,49\", \"12,24,70,49\", \"45,72\")\n// val frequency \u003d Array(2, 3, 4, 5, 1, 3)\n\n// val rows \u003d frequency.zip(trip_path)\n\n// val inputRDD \u003d sc.parallelize(rows)\n// inputRDD.collect().foreach(println)\n\n// // Convert RDD to TSV format\n// val tsvRDD \u003d inputRDD.map { case (frequency, trip_path) \u003d\u003e s\"$frequency\\t$trip_path\" }\n// tsvRDD.collect().foreach(println)\n\n// // Save RDD as TSV file\n// tsvRDD.saveAsTextFile(\"/user/cg4177_nyu_edu/step4.tsv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def loadRawDataCSV(spark: SparkSession, path: String, headers: Boolean \u003d true, inferSchema: Boolean \u003d true,\n                 delimiter: String \u003d \",\"): DataFrame \u003d {\nspark.read\n  .option(\"header\", headers)\n  .option(\"inferSchema\", inferSchema)\n  .option(\"delimiter\", delimiter)\n  .csv(path)\n}\n\n// case class TripPathFreq(frequency: BigInt, trip_path: String)\n\n// val path \u003d \"/user/wl2484_nyu_edu/project/data/intermediate/taxi_trip_path_frequency/part-00000-f0abcd69-ecbe-4170-bd8c-69830eb9ba73-c000.csv\"\n\n// val df \u003d loadRawDataCSV(spark, path, delimiter \u003d \"\\t\")\n// df.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// df.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0)))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// // // Read the .tsv file into an RDD of Strings\n// val linesRDD \u003d sc.textFile(\"/user/cg4177_nyu_edu/step4.tsv\")\n// val tripFreqRDD \u003d linesRDD.map { line \u003d\u003e\n//   // Assuming the key is the first column and the value is the second column\n//   val Array(value, key) \u003d line.split(\"\\t\")\n//   (key, value.toInt)\n// }\n// tripFreqRDD.collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// tripFreqRDD.filter(rows \u003d\u003e rows._1.contains(\"72\")).collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// val pathFreqRDDAll \u003d sc.parallelize( df.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0))).take(100))\nval pathFreqRDDAll \u003d df.rdd.map(row \u003d\u003e (row.getString(1), row.getInt(0)))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// pathFreqRDDAll.take(200).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// for testing - \n\n// val pathFreqRDD1 \u003d pathFreqRDDAll.filter(row \u003d\u003e row._1.contains(\"236,237\"))\nval pathFreqRDD2 \u003d pathFreqRDDAll.filter(row \u003d\u003e row._1.contains(\"186,100,230,163,43,41,42,120\"))\n\nval pathFreqRDD \u003d pathFreqRDDAll\n// val pathFreqRDD \u003d pathFreqRDD2\n\nval pathFreqMap: Map[String, Int] \u003d pathFreqRDD.collectAsMap()\n\nval trips \u003d pathFreqRDD.keys.collect()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "trips.length"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathFreqMap.getOrElse(\"186,100,230,163,43,41,42,120\", -1)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "trips.foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.mutable\n\n// val keysArray \u003d trips.map(key \u003d\u003e key -\u003e Array.empty[Int])\n\n// val mutableMap: mutable.Map[String, Array[Int]] \u003d mutable.Map(keysArray: _*)\n\nval mutablePathCoverageCountMap: mutable.Map[String, Array[Int]] \u003d mutable.Map.empty[String, Array[Int]]\n\npathFreqMap.foreach { case (key, value) \u003d\u003e mutablePathCoverageCountMap(key) \u003d Array(value) }"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Print the resulting mutable map\nmutablePathCoverageCountMap.foreach { case (key, values) \u003d\u003e\n  println(s\"$key -\u003e ${values.mkString(\"[\", \", \", \"]\")}\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def addIntToValueArray(key: String, valueToAdd: Int, myMap: mutable.Map[String, Array[Int]]): Unit \u003d {\n  // Get the current value array for the key, or create an empty array if the key is not present\n  val currentValueArray \u003d myMap.getOrElse(key, Array.empty[Int])\n\n  // Append the new value to the array\n  val updatedValueArray \u003d currentValueArray :+ valueToAdd\n\n  // Update the map with the new value array\n  myMap(key) \u003d updatedValueArray\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val n \u003d trips.length\n\ndef isSubsequence(s1:String, s2: String): Boolean \u003d {\n    val arr1 \u003d s1.split(\",\").map(_.toInt)\n    val arr2 \u003d s2.split(\",\").map(_.toInt)\n    var i \u003d 0  // Index for arr1\n    var j \u003d 0  // Index for arr2\n\n     while (i \u003c arr1.length \u0026\u0026 j \u003c arr2.length) {\n        if (arr1(i) \u003d\u003d arr2(j)) {\n            i +\u003d 1\n        }\n        j +\u003d 1\n    }\n\n    // If all elements of arr1 are found in arr2 in the same order\n    i \u003d\u003d arr1.length\n}\n\nfor (i \u003c- 0 until n) {\n    for (j \u003c- 0 until n) {\n        val trip1 \u003d trips(i)\n        val trip2 \u003d trips(j)\n        if (i !\u003d j) {\n            if (isSubsequence(trip1, trip2)) {\n                val key \u003d trip1\n                val valuetoAdd \u003d pathFreqMap.getOrElse(key, 0)\n                \n                addIntToValueArray(trip2, valuetoAdd, mutablePathCoverageCountMap)\n            }\n            // println(s\"Trip1: $trip1 is a subsequence of Trip2: $trip2\")\n        }\n    }\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Print the updated map\nmutablePathCoverageCountMap.foreach { case (key, values) \u003d\u003e\n  println(s\"$key -\u003e ${values.mkString(\"[\", \", \", \"]\")}\")\n  println(s\" \")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Compute coverage_count for each route\n\n\nval pathCoverageCountRDD: RDD[(String, BigInt)] \u003d sc.parallelize(\n      mutablePathCoverageCountMap.toSeq.map { case (key, values) \u003d\u003e\n        (key, BigInt(values.sum))\n      }\n    )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "case class TripPathCoverageCount(trip_path: String, coverage_count: BigInt)\nval pathCoverageCountDS: Dataset[TripPathCoverageCount] \u003d pathCoverageCountRDD.map({\n      case (trip_path, coverage_count) \u003d\u003e TripPathCoverageCount(trip_path, coverage_count)}).toDS()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathCoverageCountDS.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathCoverageCountDS.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pathFreqMap.getOrElse(\"236,237\", -1)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "mutablePathCoverageCountMap(\"236,237\")"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def savePathCoverageOutput(pathCoverageCountDS: Dataset[TripPathCoverageCount], path: String): Unit \u003d {\n    val toSaveDS \u003d pathCoverageCountDS.select(\"coverage_count\", \"trip_path\").as[TripPathCoverageCount]\n    toSaveDS.coalesce(1)\n      .orderBy(desc(\"coverage_count\"))\n      .write\n      .mode(SaveMode.Overwrite) // workaround for abnormal path-already-exists error\n      .option(\"header\", true)\n      .option(\"delimiter\", \"\\t\")\n      .csv(f\"$path\")\n  }"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pathCoverageOutputPath \u003d \"/user/wl2484_nyu_edu/project/data/intermediate/trip_paths_coverage_count\"\nsavePathCoverageOutput(pathCoverageCountDS, pathCoverageOutputPath)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Step-5"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// verify - \n\nval resultDF \u003d loadRawDataCSV(spark, \"/user/wl2484_nyu_edu/project/data/intermediate/trip_paths_coverage_count\", delimiter \u003d \"\\t\")"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "resultDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "resultDF.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lookupPath \u003d \"/user/wl2484_nyu_edu/project/data/source/tlc/zone_lookup\"\n\nval lookupDF \u003d loadRawDataCSV(spark, lookupPath, delimiter\u003d\",\")\nlookupDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lookupMap: Map[Int, String] \u003d  lookupDF.withColumn(\"Zone1\", split(col(\"Zone\"), \"/\")(0)).select(\"LocationID\", \"Zone1\").as[(Int, String)].rdd.collectAsMap()"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def mapLocationIds(input: String, locationIdZoneMap: Map[Int, String]): String \u003d {\n    input.split(\",\").map(num \u003d\u003e locationIdZoneMap.getOrElse(num.toInt, \"NA\")).mkString(\",\")\n}\n\nval mapLocationIdsUDF \u003d udf((input: String) \u003d\u003e mapLocationIds(input, lookupMap))\n\n      \nval resultDF1 \u003d resultDF.withColumn(\"route\", mapLocationIdsUDF(col(\"trip_path\")))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "resultDF1.select()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val k \u003d 10\nval m \u003d 10\nval out \u003d topKTripsWithAtleastMStops(10, 10, resultDF).toDF()"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "out.take(3).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def topKTripsWithAtleastMStops(k: Int, m: Int, df: DataFrame): Dataset[TripPathCoverageCount] \u003d {\n    import spark.implicits._\n    val filteredDS: Dataset[TripPathCoverageCount] \u003d df.as[TripPathCoverageCount].filter({ data \u003d\u003e data.trip_path.split(\",\").length \u003e\u003d m })\n    val sortedDS: Dataset[TripPathCoverageCount] \u003d filteredDS.orderBy($\"coverage_count\".desc)\n    sortedDS.limit(k).as[TripPathCoverageCount]\n  }"
    }
  ]
}